import torch
import matplotlib.pyplot as plt

def debug_forward_output(y_pred, y_input):
    """æ£€æŸ¥å‰å‘ä¼ æ’­è¾“å‡ºæ˜¯å¦æ­£å¸¸"""
    print(f"[Forward] y_pred: mean={y_pred.mean():.4f}, std={y_pred.std():.4f}, min={y_pred.min():.4f}, max={y_pred.max():.4f}")
    print(f"[Forward] y_input: mean={y_input.mean():.4f}, std={y_input.std():.4f}, min={y_input.min():.4f}, max={y_input.max():.4f}")
    if torch.isnan(y_pred).any() or torch.isinf(y_pred).any():
        print("âš ï¸ Forward output contains NaN or Inf!")

def check_gradient_none(model):
    """æ£€æŸ¥æ˜¯å¦æœ‰å‚æ•°æ²¡æœ‰æ¢¯åº¦ä¼ æ’­"""
    for name, param in model.named_parameters():
        if param.requires_grad and param.grad is None:
            print(f"ğŸš¨ [None Grad] {name} has no gradient!")

def print_gradient_stats(model):
    """æ‰“å°æ¯ä¸€å±‚å‚æ•°æ¢¯åº¦çš„ç»Ÿè®¡å€¼"""
    print("ğŸ” Gradient stats per parameter:")
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad = param.grad
            print(f"{name:40s} | mean: {grad.mean():.6f}, std: {grad.std():.6f}, max: {grad.max():.6f}, min: {grad.min():.6f}")

def visualize_gradients(model, title="Gradient Distribution"):
    """ç»˜åˆ¶æ‰€æœ‰å‚æ•°æ¢¯åº¦çš„ç›´æ–¹å›¾"""
    grads = []
    for name, param in model.named_parameters():
        if param.grad is not None:
            grads.append(param.grad.view(-1).detach().cpu())
    if grads:
        all_grads = torch.cat(grads)
        plt.figure(figsize=(6, 4))
        plt.hist(all_grads, bins=100, alpha=0.7, color='steelblue')
        plt.title(title)
        plt.xlabel("Gradient value")
        plt.ylabel("Frequency")
        plt.grid(True)
        plt.tight_layout()
        plt.show()

def run_debug_checks(model, y_pred, y_input, debug=False):
    if debug:
        debug_forward_output(y_pred, y_input)
        check_gradient_none(model)
        print_gradient_stats(model)
        # visualize_gradients(model)
